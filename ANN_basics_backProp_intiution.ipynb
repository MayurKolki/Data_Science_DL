{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "ANN_basics_backProp_intiution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MayurKolki/Data_Science_DL/blob/main/ANN_basics_backProp_intiution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxWPi0qVKfuL"
      },
      "source": [
        "## Multi-layer Perceptron\n",
        "\n",
        "\n",
        "The solution to fitting more complex (*i.e.* non-linear) models with neural networks is to use a more complex network that consists of more than just a single perceptron. The take-home message from the perceptron is that all of the learning happens by adapting the synapse weights until prediction is satisfactory. Hence, a reasonable guess at how to make a perceptron more complex is to simply **add more weights**.\n",
        "\n",
        "There are two ways to add complexity:\n",
        "\n",
        "1. Add backward connections, so that output neurons feed back to input nodes, resulting in a **recurrent network**\n",
        "2. Add neurons between the input nodes and the outputs, creating an additional (\"hidden\") layer to the network, resulting in a **multi-layer perceptron**\n",
        "\n",
        "The latter approach is more common in applications of neural networks.\n",
        "\n",
        "<a href=\"https://i.stack.imgur.com/n2Hde.png\">image source</a>\n",
        "\n",
        "<img src=\"https://i.stack.imgur.com/n2Hde.png\" width=50%>\n",
        "\n",
        "How to train a multilayer network is not intuitive. Propagating the inputs forward over two layers is straightforward, since the outputs from the hidden layer can be used as inputs for the output layer. However, the process for updating the weights based on the prediction error is less clear, since it is difficult to know whether to change the weights on the input layer or on the hidden layer in order to improve the prediction.\n",
        "\n",
        "Updating a multi-layer perceptron (MLP) is a matter of: \n",
        "\n",
        "1. moving forward through the network, calculating outputs given inputs and current weight estimates\n",
        "2. moving backward updating weights according to the resulting error from forward propagation. \n",
        "\n",
        "In this sense, it is similar to a single-layer perceptron, except it has to be done twice, once for each layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGeIe5StKfuO"
      },
      "source": [
        "## Backpropagation intiution\n",
        "\n",
        "* In the year 1986 a groundbreaking paper \"Learning Internal Representation by Error Propagation\" was published by -\n",
        "    * David Rumelhart,\n",
        "    * Geoffrey Hinton, &\n",
        "    * Ronald Williams \n",
        "    \n",
        "* It depicted an efficient way to update weights and biases of the network based on the error/loss function by passing twice through the network i.e forward and backward pass.\n",
        "    - forward pass: data is passed through the input layer to the hidden layer and it calculates ouput. Its nothing but making prediction.\n",
        "    - error calculation: Based on loss function error is calculated to check how much deviation is there from the ground truth or actual value and predicted value.\n",
        "    - error contribution from the each connection of the output layer is calculated.\n",
        "    - Then algo goes a layer deep and calculates how much previous layer contributed into the error of present layer and this way it propagates till the input layer.\n",
        "    - This reverse pass measures the error gradient accross all the connection.\n",
        "    - At last by using these error gradients a gradient step is performed to update the weights.\n",
        "    \n",
        "* In MLP key changes were to introduce a sigmoid activation function $$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMVvnYKNKfuQ",
        "outputId": "e838e6bb-c028-453f-f98c-1228ae8c3e65"
      },
      "source": [
        "from IPython.display import IFrame\n",
        "\n",
        "IFrame(\"https://slides.com/supremecommander/basic-neural-network/embed\",\n",
        "      width=576, height=420)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"576\"\n",
              "            height=\"420\"\n",
              "            src=\"https://slides.com/supremecommander/basic-neural-network/embed\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x1494791b908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pa9h8TsKfuX"
      },
      "source": [
        "## Need of activation function\n",
        "\n",
        "* No activation function => deep stack of network will behave like a single linear transformation.\n",
        "* Without activation function all the continuous function cannot be approximated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNzzXNF_KfuY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}